{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465564be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "from pyproj import Transformer, CRS, Proj\n",
    "from shapely.geometry import shape, Point, Polygon, box\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "import shapely.speedups as speedups\n",
    "import contextily as ctx\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import mapclassify as mc\n",
    "speedups.enable()\n",
    "import re\n",
    "from python_hll.hll import HLL\n",
    "import mmh3\n",
    "from python_hll.util import NumberUtil\n",
    "import emoji\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants to be used throughout the program\n",
    "\n",
    "#create grids based on the study area shapefile\n",
    "\n",
    "GRID_SIZE_METERS = 100000\n",
    "                        \n",
    "# target projection: Mollweide\n",
    "EPSG_CODE = 54009\n",
    "CRS_PROJ = f\"esri:{EPSG_CODE}\"\n",
    "\n",
    "# Input projection WGS 84\n",
    "CRS_WGS = \"epsg:4326\"\n",
    "\n",
    "# define Transformer to project things to Mollweide\n",
    "PROJ_TRANSFORMER = Transformer.from_crs(\n",
    "    CRS_WGS, CRS_PROJ, always_xy=True)\n",
    "\n",
    "# also define reverse projection\n",
    "PROJ_TRANSFORMER_BACK = Transformer.from_crs(\n",
    "    CRS_PROJ, CRS_WGS, always_xy=True)\n",
    "\n",
    "#projecting the bounds of the study area shapefile to Mollweide\n",
    "XMIN = PROJ_TRANSFORMER.transform(-18.729512 , 29.234046)[0]\n",
    "XMAX = PROJ_TRANSFORMER.transform(39.73858, 29.234046)[0]\n",
    "YMAX = PROJ_TRANSFORMER.transform(49.59352369, 71.16987838)[1]\n",
    "YMIN = PROJ_TRANSFORMER.transform(49.59352369, 28.017169)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a900e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the study area shapefile\n",
    "europe = gp.read_file(\"Europe_Clipped_BBox.shp\")\n",
    "europe.to_crs(CRS_PROJ, inplace =True)\n",
    "europe.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9cbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "europe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grids():\n",
    "    \n",
    "#     Creating polygons based on the grid size\n",
    "    \n",
    "    width = GRID_SIZE_METERS\n",
    "    length = GRID_SIZE_METERS\n",
    "    cols = list(range(int(np.floor(XMIN)), int(np.ceil(XMAX)), width))\n",
    "    rows = list(range(int(np.floor(YMIN)), int(np.ceil(YMAX)), length))\n",
    "    rows.reverse()\n",
    "\n",
    "    polygons = []\n",
    "    for x in cols:\n",
    "         for y in rows:\n",
    "                # combine to tuple: (x,y, poly)\n",
    "                # and append to list\n",
    "                polygons.append(\n",
    "                    (x, y,\n",
    "                     Polygon([\n",
    "                         (x, y),\n",
    "                         (x+width, y),\n",
    "                         (x+width, y-length),\n",
    "                         (x, y-length)])))\n",
    "    grid = pd.DataFrame(polygons)\n",
    "        # name columns\n",
    "    col_labels=['xbin', 'ybin', 'bin_poly']\n",
    "    grid.columns = col_labels\n",
    "        # use x and y as index columns\n",
    "    grid.set_index(['xbin', 'ybin'], inplace=True)\n",
    "    grid = gp.GeoDataFrame(\n",
    "            grid.drop(\n",
    "                columns=[\"bin_poly\"]),\n",
    "                geometry=grid.bin_poly)\n",
    "    grid.crs = CRS_PROJ\n",
    "    return grid,cols,rows\n",
    "\n",
    "grid,cols,rows = create_grids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_grid = grid.centroid.reset_index()\n",
    "centroid_grid.set_index([\"xbin\", \"ybin\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30490aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a91470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopandas.tools import sjoin\n",
    "def intersect_grid_centroids(\n",
    "    grid: gp.GeoDataFrame, \n",
    "    intersect_gdf: gp.GeoDataFrame):\n",
    "    \"\"\"Return grid centroids from grid that \n",
    "    intersect with intersect_gdf\n",
    "    \"\"\"\n",
    "    centroid_grid = gp.GeoDataFrame(\n",
    "        grid.centroid)\n",
    "    centroid_grid.rename(\n",
    "        columns={0:'geometry'},\n",
    "        inplace=True)\n",
    "    centroid_grid.set_geometry(\n",
    "        'geometry', crs=grid.crs, \n",
    "        inplace=True)\n",
    "    grid_intersect = sjoin(\n",
    "        centroid_grid, intersect_gdf, \n",
    "        how='right')\n",
    "    grid_intersect.set_index(\n",
    "        [\"index_left0\", \"index_left1\"],\n",
    "        inplace=True)\n",
    "    grid_intersect.index.names = ['xbin','ybin']\n",
    "    return grid.loc[grid_intersect.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c3504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid.boundary.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac0b9c",
   "metadata": {},
   "source": [
    "#### There are several different aggregation levels avaialble to use with HLL data depending on how much we would like to reduce spatial accuracy and therefore preserve user privacy. Let's plot a few to see how the aggregation levels differ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c561c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and read csv file containing HLL data aggregated to level 5\n",
    "df5 = pd.read_csv(r\"C:\\Users\\saman\\OneDrive\\Documents\\Thesis\\Data\\HLL_Data_Final_5.csv\")\n",
    "# Convert to geodataframe, projection WGS84\n",
    "gdf5 = gp.GeoDataFrame(df5,geometry =gp.points_from_xy(df5.longitude_5,df5.latitude_5),crs =4326)\n",
    "gdf5.to_crs(CRS_PROJ,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc9a408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the points on top of study area\n",
    "fig, ax = plt.subplots(figsize=(35, 20))\n",
    "grid.plot(ax=ax, color='white', edgecolor='black', linewidth=0.1)\n",
    "europe.boundary.plot(ax=ax, color=\"black\", linewidth=0.8)\n",
    "gdf5.plot(ax=ax, color=\"purple\", markersize=3)\n",
    "ax.set_title(\"Twitter Post Locations (Aggregated to Level 5)\", fontsize=20)\n",
    "ax.set_axis_off()\n",
    "plt.show()\n",
    "fig.savefig(r\"C:\\Users\\saman\\OneDrive\\Documents\\Thesis\\Figures\\AllPosts_mapped_5_grid.png\", dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2241638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and read csv file containing HLL data aggregated to level 4\n",
    "df4 = pd.read_csv(r\"C:\\Users\\saman\\OneDrive\\Documents\\Thesis\\Data\\HLL_Data_Final_4.csv\")\n",
    "# Convert to geodataframe, projection WGS84\n",
    "gdf4 = gp.GeoDataFrame(df4,geometry=gp.points_from_xy(df4.longitude_4,df4.latitude_4),crs =4326)\n",
    "gdf4.to_crs(CRS_PROJ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6338b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(35, 20))\n",
    "grid.plot(ax=ax, color='white', edgecolor='black', linewidth=0.1)\n",
    "europe.boundary.plot(ax=ax, color=\"black\", linewidth=0.8)\n",
    "gdf4.plot(ax=ax, color=\"purple\", markersize=3)\n",
    "ax.set_title(\"Twitter Post Locations (Aggregated to Level 4)\", fontsize=20)\n",
    "ax.set_axis_off()\n",
    "plt.show()\n",
    "fig.savefig(r\"C:\\Users\\saman\\OneDrive\\Documents\\Thesis\\Figures\\AllPosts_mapped_4_grid.png\", dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and read csv file containing HLL data aggregated to level 3\n",
    "df3 = pd.read_csv (r\"C:\\Users\\saman\\OneDrive\\Documents\\Thesis\\Data\\HLL_Data_Final_3.csv\")\n",
    "# Convert to geodataframe, projection WGS84\n",
    "gdf3 = gp.GeoDataFrame(df3,geometry =gp.points_from_xy(df3.longitude_3,df3.latitude_3),crs =4326)\n",
    "# project gdf to Mollweide\n",
    "gdf3.to_crs(CRS_PROJ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e532df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(35, 20))\n",
    "grid.plot(ax=ax, color='white', edgecolor='black', linewidth=0.1)\n",
    "europe.boundary.plot(ax=ax, color=\"black\", linewidth=0.8)\n",
    "gdf3.plot(ax=ax, color=\"purple\", markersize=10)\n",
    "ax.set_title(\"Twitter Post Locations (Aggregated to Level 3)\", fontsize=20)\n",
    "ax.set_axis_off()\n",
    "plt.show()\n",
    "fig.savefig(r\"C:\\Users\\saman\\OneDrive\\Documents\\Thesis\\Figures\\AllPosts_mapped_3_grid.png\", dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d84f5",
   "metadata": {},
   "source": [
    "#### It looks like an aggregation level of 4 is a good balance between spatial accuracy and privacy preservation. An aggregation level of 5 seems to not have a significant benefit for privacy and level 3 poses issues where the data has such a coarse resolution that it creates some blank grid cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc338ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf4\n",
    "# rename lat and long columns to avoid confusion/simplify things\n",
    "gdf.rename(columns = {'latitude_4':'latitude', 'longitude_4':'longitude'}, inplace = True)\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before starting our analysis, let's double-check to make sure every row has an emoji \n",
    "# (no blank rows due to flags rendering incorrectly, etc.)\n",
    "gdf_noemoji = gdf[gdf['emoji'] =='']\n",
    "gdf_noemoji.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ff5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to join HLL post info with these grid cells using np.digitize\n",
    "ybins = np.array(rows)\n",
    "xbins = np.array(cols)\n",
    "\n",
    "def get_best_bins(search_values_x, search_values_y,xbins, ybins): \n",
    "    \"\"\"Will return best bin for a lat and lng input\n",
    "    \n",
    "    Note: prepare bins and values in correct matching projection\n",
    "    \n",
    "    \"\"\"\n",
    "    xbins_idx = np.digitize(search_values_x, xbins, right=False)\n",
    "    ybins_idx = np.digitize(search_values_y, ybins, right=False)\n",
    "    return (xbins[xbins_idx-1], ybins[ybins_idx-1])\n",
    "\n",
    "\n",
    "xbins_match, ybins_match = get_best_bins(\n",
    "    search_values_x=gdf.geometry.x.to_numpy(),\n",
    "    search_values_y=gdf.geometry.y.to_numpy(),\n",
    "    xbins=xbins, ybins=ybins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366ecf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf.loc[:, 'xbins_match'] = xbins_match\n",
    "gdf.loc[:, 'ybins_match'] = ybins_match\n",
    "gdf.set_index(['xbins_match', 'ybins_match'], inplace=True)\n",
    "grid.sort_index(inplace =True)\n",
    "gdf.sort_index(inplace = True)\n",
    "common_idx = grid.index.intersection(gdf.index) \n",
    "#instead of a spatial join, indexes are used to find which hashtag belongs to which grid\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36509173",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def hll_from_byte(hll_set: str):\n",
    "# Return HLL set from binary representation\n",
    "    hex_string = hll_set[2:]\n",
    "    return HLL.from_bytes(\n",
    "        NumberUtil.from_hex(\n",
    "            hex_string, 0, len(hex_string)))\n",
    "def cardinality_from_hll(hll_set):\n",
    "# Turn binary hll into HLL set and return cardinality\n",
    "    hll = hll_from_byte(hll_set)\n",
    "    return hll.cardinality() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73193637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def union_hll(hll: HLL, hll2):\n",
    "    \"\"\"Union of two HLL sets. The first HLL set will be modified in-place.\"\"\"\n",
    "    hll.union(hll2)\n",
    "    \n",
    "def union_all_hll(\n",
    "    hll_series: pd.Series, cardinality: bool = True) -> pd.Series:\n",
    "    \"\"\"HLL Union and (optional) cardinality estimation from series of hll sets\n",
    "\n",
    "        Args:\n",
    "        hll_series: Indexed series (bins) of hll sets. \n",
    "        cardinality: If True, returns cardinality (counts). Otherwise,\n",
    "            the unioned hll set will be returned.\n",
    "    \"\"\"\n",
    "    \n",
    "    hll_set = None\n",
    "    for hll_set_str in hll_series.values.tolist():\n",
    "        if hll_set is None:\n",
    "            # set first hll set\n",
    "            hll_set = hll_from_byte(hll_set_str)\n",
    "            continue\n",
    "        hll_set2 = hll_from_byte(hll_set_str)\n",
    "        union_hll(hll_set, hll_set2)\n",
    "    return hll_set.cardinality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe2d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_userdays(new_test,idx):    \n",
    "        counter = Counter(\n",
    "        n_s = union_all_hll(gdf.loc[idx,'pud_hll'].dropna())\n",
    "        ud.loc[idx,'pud_hll'] = n_s\n",
    "\n",
    "ud = pd.DataFrame(index = common_idx, columns = ['pud_hll'], data = '') #dummy dataframe to hold the typicality values\n",
    "\n",
    "for idx,midx in enumerate(common_idx): #looping through all the common indexes between the grids and dataframe\n",
    "    grid_userdays(gdf.loc[midx,\"pud_hll\"], common_idx[idx])\n",
    "\n",
    "geom = grid.loc[common_idx, \"geometry\"]\n",
    "ud_gdf = gp.GeoDataFrame(data = ud['pud_hll'], geometry =geom, crs = CRS_PROJ)\n",
    "ud_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea52db",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 500, 2000, 5000, 10000, 200000\n",
    "base = grid.plot(figsize=(22,28), color='white', edgecolor='black', linewidth=0.1)\n",
    "plot = ud_gdf.plot(ax=base, colormap='Purples', column='pud_hll', alpha = 0.7, edgecolor='gray', linewidth=0.1, legend=True,\n",
    "                   scheme='UserDefined',\n",
    "                   classification_kwds={'bins': bins})\n",
    "# plot.legend(prop={'size': 6})\n",
    "europe.boundary.plot(ax=base, edgecolor='dimgray', linewidth=0.7,)\n",
    "plt.title(\"Distribution of User Days\", size =35)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig(r\"C:\\Users\\saman\\OneDrive\\Documents\\Thesis\\Figures\\Total_User_Days.png\", dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7eb4f8",
   "metadata": {},
   "source": [
    "### Theoretically, we could calculate the number of userdays for every country present in the dataset. This is unfortunately too computationally intensive and time consuming for the scope of this project, so total userdays were only calculated for a subset of countries based on their coverage with the available dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17898588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dictionary to fill with countries and their total userdays\n",
    "country_ud = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk = europe[europe['NAME_EN'] == 'United Kingdom']\n",
    "# make country grid\n",
    "grid_uk = intersect_grid_centroids(\n",
    "    grid=grid, intersect_gdf=uk)\n",
    "# join hll data to grid\n",
    "uk_hll = gdf.sjoin(grid_uk, how=\"right\")\n",
    "# join together country grid cells, calculate the number of userdays\n",
    "# within the cluster of grid cells\n",
    "userdays_uk = union_all_hll(uk_hll[\"pud_hll\"].dropna())\n",
    "# add the result to a dictionary\n",
    "country_ud[\"United Kingdom\"] = userdays_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c11bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = europe[europe['NAME_EN'] == 'France']\n",
    "grid_fr = intersect_grid_centroids(\n",
    "    grid=grid, intersect_gdf=fr)\n",
    "fr_hll = gdf.sjoin(grid_fr, how=\"right\")\n",
    "userdays_fr = union_all_hll(fr_hll[\"pud_hll\"].dropna())\n",
    "country_ud[\"France\"] = userdays_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = europe[europe['NAME_EN'] == 'Spain']\n",
    "grid_sp = intersect_grid_centroids(\n",
    "    grid=grid, intersect_gdf=sp)\n",
    "sp_hll = gdf.sjoin(grid_sp, how=\"right\")\n",
    "userdays_sp = union_all_hll(sp_hll[\"pud_hll\"].dropna())\n",
    "country_ud[\"Spain\"] = userdays_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = europe[europe['NAME_EN'] == 'Italy']\n",
    "grid_it = intersect_grid_centroids(\n",
    "    grid=grid, intersect_gdf=it)\n",
    "it_hll = gdf.sjoin(grid_it, how=\"right\")\n",
    "userdays_it = union_all_hll(it_hll[\"pud_hll\"].dropna())\n",
    "country_ud[\"Italy\"] = userdays_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "de = europe[europe['NAME_EN'] == 'Germany']\n",
    "grid_de = intersect_grid_centroids(\n",
    "    grid=grid, intersect_gdf=de)\n",
    "de_hll = gdf.sjoin(grid_de, how=\"right\")\n",
    "userdays_de = union_all_hll(de_hll[\"pud_hll\"].dropna())\n",
    "country_ud[\"Germany\"] = userdays_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ebaa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ne = europe[europe['NAME_EN'] == 'Netherlands']\n",
    "grid_ne = intersect_grid_centroids(\n",
    "    grid=grid, intersect_gdf=ne)\n",
    "ne_hll = gdf.sjoin(grid_ne, how=\"right\")\n",
    "userdays_ne = union_all_hll(ne_hll[\"pud_hll\"].dropna())\n",
    "country_ud[\"Netherlands\"] = userdays_ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0278b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "tur = europe[europe['NAME_EN'] == 'Turkey']\n",
    "grid_tur = intersect_grid_centroids(\n",
    "    grid=grid, intersect_gdf=tur)\n",
    "tur_hll = gdf.sjoin(grid_tur, how=\"right\")\n",
    "userdays_tur = union_all_hll(tur_hll[\"pud_hll\"].dropna())\n",
    "country_ud[\"Turkey\"] = userdays_tur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a695eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cz = europe[europe['NAME_EN'] == 'Czech Republic']\n",
    "grid_cz = intersect_grid_centroids(\n",
    "    grid=grid, intersect_gdf=cz)\n",
    "cz_hll = gdf.sjoin(grid_cz, how=\"right\")\n",
    "userdays_cz = union_all_hll(cz_hll[\"pud_hll\"].dropna())\n",
    "country_ud[\"Czech Republic\"] = userdays_cz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "be = europe[europe['NAME_EN'] == 'Belgium']\n",
    "grid_be = intersect_grid_centroids(\n",
    "    grid=grid, intersect_gdf=be)\n",
    "be_hll = gdf.sjoin(grid_be, how=\"right\")\n",
    "userdays_be = union_all_hll(be_hll[\"pud_hll\"].dropna())\n",
    "country_ud[\"Belgium\"] = userdays_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09323ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = europe[europe['NAME_EN'] == 'Switzerland']\n",
    "grid_sw = intersect_grid_centroids(\n",
    "    grid=grid, intersect_gdf=sw)\n",
    "sw_hll = gdf.sjoin(grid_sw, how=\"right\")\n",
    "userdays_sw = union_all_hll(sw_hll[\"pud_hll\"].dropna())\n",
    "country_ud[\"Switzerland\"] = userdays_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bfc7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "po = europe[europe['NAME_EN'] == 'Portugal']\n",
    "grid_po = intersect_grid_centroids(\n",
    "    grid=grid, intersect_gdf=po)\n",
    "po_hll = gdf.sjoin(grid_po, how=\"right\")\n",
    "userdays_po = union_all_hll(po_hll[\"pud_hll\"].dropna())\n",
    "userdays_po\n",
    "country_ud[\"Portugal\"] = userdays_po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901df553",
   "metadata": {},
   "outputs": [],
   "source": [
    "au = europe[europe['NAME_EN'] == 'Austria']\n",
    "grid_au = intersect_grid_centroids(\n",
    "    grid=grid, intersect_gdf=au)\n",
    "au_hll = gdf.sjoin(grid_au, how=\"right\")\n",
    "userdays_au = union_all_hll(au_hll[\"pud_hll\"].dropna())\n",
    "country_ud[\"Austria\"] = userdays_au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1be8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This dictionary is the result of the above code when run on HLL data with an aggregation level of 4.\n",
    "\"\"\"\n",
    "country_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d893bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following dictionary was the result of calculating the cardinality of userdays per country while using an aggregation \n",
    "level of 5. Between these and the previous results, the error rate of HLL (3-5%) is clearly illustrated.\n",
    "\"\"\"\n",
    "country_ud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b7c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_ud_sorted = sorted(country_ud.items(), key=operator.itemgetter(1), reverse=True)\n",
    "country_ud_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4551d",
   "metadata": {},
   "source": [
    "top 10 countries by userdays are:\n",
    "\n",
    "United Kingdom\n",
    "Spain\n",
    "France\n",
    "Germany\n",
    "Italy\n",
    "Turkey\n",
    "Netherlands\n",
    "Belgium\n",
    "Switzerland\n",
    "Austria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also calculate the total number of userdays across the entire study area\n",
    "total_userdays = union_all_hll(gdf[\"pud_hll\"].dropna())\n",
    "total_userdays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57583c83",
   "metadata": {},
   "source": [
    "## Experiment using unions to combine emojis with different skin tones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb87d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbsupgdf = gdf[gdf['emoji'].str.contains('👍|👍🏻|👍🏼|👍🏽|👍🏾|👍🏿')]\n",
    "thumbsupgdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if we can now find userdays per emoji\n",
    "thumbsup_ud = union_all_hll(thumbsupgdf[\"pud_hll\"].dropna())\n",
    "thumbsup_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cee2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare that value for the number of userdays for just the generic (yellow) thumbs up emoji\n",
    "thumbsup_gen_gdf = gdf[gdf['emoji']=='👍']\n",
    "thumbsup_gen_ud = union_all_hll(thumbsup_gen_gdf[\"pud_hll\"].dropna())\n",
    "thumbsup_gen_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# great - it works! now I'll repeat these for all skin-tone emojis in the list of top 50 most frequently ocurring emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e38e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "prayhandsgdf = gdf[gdf['emoji'].str.contains('🙏|🙏🏻|🙏🏼|🙏🏽|🙏🏾|🙏🏿')]\n",
    "prayhands_ud = union_all_hll(prayhandsgdf[\"pud_hll\"].dropna())\n",
    "prayhands_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "clapgdf = gdf[gdf['emoji'].str.contains('👏|👏🏻|👏🏼|👏🏽|👏🏾|👏🏿')]\n",
    "clap_ud = union_all_hll(clapgdf[\"pud_hll\"].dropna())\n",
    "clap_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce5bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary of emoji names and their variations\n",
    "emojidictemo = {\n",
    "    \":clapping_hands:\": \"👏|👏🏻|👏🏼|👏🏽|👏🏾|👏🏿\",\n",
    "    \":folded_hands:\": \"🙏|🙏🏻|🙏🏼|🙏🏽|🙏🏾|🙏🏿\",\n",
    "    \":thumbs_up:\": \"👍|👍🏻|👍🏼|👍🏽|👍🏾|👍🏿\",\n",
    "    \":flexed_biceps:\": \"💪|💪🏻|💪🏼|💪🏽|💪🏾|💪🏿\",\n",
    "    \":OK_hand:\": \"👌|👌🏻|👌🏼|👌🏽|👌🏾|👌🏿\",\n",
    "    \":raising_hands:\": \"🙌|🙌🏻|🙌🏼|🙌🏽|🙌🏾|🙌🏿\",\n",
    "    \":backhand_index_pointing_down:\": \"👇|👇🏻|👇🏼|👇🏽|👇🏾|👇🏿\",\n",
    "    \":backhand_index_pointing_right:\": \"👉|👉🏻|👉🏼|👉🏽|👉🏾|👉🏿\",\n",
    "    \":victory_hand:\": \"✌️|✌🏻|✌🏼|✌🏽|✌🏾|✌🏿\",\n",
    "    \":oncoming_fist:\": \"👊|👊🏻|👊🏼|👊🏽|👊🏾|👊🏿\"\n",
    "}\n",
    "\n",
    "emoji_ud = {}\n",
    "\n",
    "for name, variations in emojidictemo.items():\n",
    "    subset = gdf[gdf['emoji'].str.contains(variations)]\n",
    "    emoji_ud[name] = union_all_hll(subset[\"pud_hll\"].dropna())\n",
    "\n",
    "emoji_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a65775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's run it for the top 50 most common emojis by absolute frequency and compare user days to post count\n",
    "# there should be a huge difference for emojis primarily used by bots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfeb86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topemojisdf = gdf['emoji'].value_counts()\n",
    "top100list = topemojisdf.head(100)\n",
    "top100dict = top100list.to_dict()\n",
    "\n",
    "emoji_userdays_100 = {}\n",
    "top100 = top100dict.keys()\n",
    "for emoj in top100:\n",
    "    genericemoji = emoji.demojize(emoj).replace(\"_light_skin_tone\",\"\").replace(\"_medium-light_skin_tone\",\"\").replace(\"_medium_skin_tone\",\"\").replace(\"_medium-dark_skin_tone\",\"\").replace(\"_dark_skin_tone\",\"\")\n",
    "    if genericemoji in emojidictemo.keys():\n",
    "        emo = emojidictemo[genericemoji]\n",
    "    else:\n",
    "        emo = emoji.emojize(genericemoji)\n",
    "    emokey = emoji.emojize(genericemoji) # this should avoid any long lists in the dictionary keys\n",
    "    subset = gdf[gdf['emoji'].str.contains(emo)]\n",
    "    emoji_userdays_100[emoj] = union_all_hll(subset[\"pud_hll\"].dropna())\n",
    "\n",
    "emoji_userdays_100    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8433193",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_userdays_100 = {'❤️': 187938, '😍': 130963, '🔴': 34552, '😂': 190683, '😎': 58073, '☀️': 44965, '😊': 69271, \n",
    "                      '💙': 82002, '😉': 65919, '🥰': 59354, '🔥': 66638, '👍': 95117, '🤣': 82473, '😁': 54824, \n",
    "                      '🚓': 2050, '💪': 87392, '❤': 187938, '📸': 33003, '🙏': 93921, '🤩': 35565, '✨': 38818, \n",
    "                      '💚': 41228, '🤔': 52586, '👏': 99485, '🎉': 29751, '😋': 27232, '🚗': 3572, '😘': 37036, \n",
    "                      '🖤': 37687, '👌': 50397, '🌞': 20908, '💛': 45365, '♥️': 33384, '😅': 35820, '💕': 31422,\n",
    "                      '🤗': 30618, '🎶': 29636, '💜': 37766, '😀': 27025, '🎄': 20375, '🥳': 24477, '😜': 24255, \n",
    "                      '🌊': 14894, '😭': 39637, '🌈': 26916, '⚽️': 40216, '🥂': 15282, '😷': 21079, '✅': 22893, \n",
    "                      '👀': 27175, '📷': 15527, '🤪': 20030, '🙄': 31538, '😃': 18466, '🙈': 24852, '💥': 20408, \n",
    "                      '😱': 22194, '❄️': 11336, '💖': 19160, '💪🏻': 87392, '☺️': 16571, '🙌': 45654, '🌸': 16146, \n",
    "                      '😢': 20462, '🤍': 17627, '🌹': 20112, '🚒': 3050, '🏆': 16975, '🍻': 11991, '🍀': 17220, \n",
    "                      '🐶': 12552, '📍': 12179, '🧡': 15547, '😳': 20269, '👉': 26892, '😄': 14377, '💯': 15759, \n",
    "                      '💫': 14460, '🍷': 11211, '🚑': 3789, '🙂': 14460, '👑': 14204, '👏🏻': 99485, '➡️': 14741, \n",
    "                      '🙏🏻': 93921, '🌿': 11228, '🍾': 9812, '😏': 15820, '👍🏻': 95117, '🌟': 13790, '😬': 15466, \n",
    "                      '🌅': 7918, '🎂': 10607, '🌳': 9174, '👇': 36136, '🎁': 10240, '🔝': 11544, '🔵': 15461,\n",
    "                      '😆': 13585, '🌍': 11352}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a4943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_emojis_ud = sorted(emoji_userdays_100.items(), key=operator.itemgetter(1), reverse=True)\n",
    "top_emojis_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a919ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emoji_postcount_100 = {}\n",
    "top100 = top100dict.keys()\n",
    "for emoj in top100:\n",
    "    genericemoji = emoji.demojize(emoj).replace(\"_light_skin_tone\",\"\").replace(\"_medium-light_skin_tone\",\"\").replace(\"_medium_skin_tone\",\"\").replace(\"_medium-dark_skin_tone\",\"\").replace(\"_dark_skin_tone\",\"\")\n",
    "    if genericemoji in emojidictemo.keys():\n",
    "        emo = emojidictemo[genericemoji]\n",
    "    else:\n",
    "        emo = emoji.emojize(genericemoji)\n",
    "    emokey = emoji.emojize(genericemoji) # this should avoid any long lists in the dictionary keys\n",
    "    subset = gdf[gdf['emoji'].str.contains(emo)]\n",
    "    emoji_postcount_100[emoj] = union_all_hll(subset[\"post_hll\"].dropna())\n",
    "\n",
    "emoji_postcount_100    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffdict = {}\n",
    "for emo in emoji_userdays_100.keys():\n",
    "    diffdict[emo] = emoji_postcount_100[emo] - emoji_userdays_100[emo]\n",
    "diffdict_sorted = sorted(diffdict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "diffdict_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
